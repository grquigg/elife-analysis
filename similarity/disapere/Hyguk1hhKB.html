<HTML>   <head>      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">      <link rel="stylesheet" href="css/dataTables.bulma.min.css">      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">   </head>   <body>      <div class="container">         <div class="columns">            <div class="column is-half">               <table border="1" class="table">  <thead>    <tr style="text-align: right;">      <th></th>      <th>review_sentence</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1. I had hard time to understand latent canonicalization.</td>    </tr>    <tr>      <th>1</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>    </tr>    <tr>      <th>2</th>      <td>More explanation of canonicalization is needed.</td>    </tr>    <tr>      <th>3</th>      <td>Perhaps an example in linear algebra is needed.</td>    </tr>    <tr>      <th>4</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>    </tr>    <tr>      <th>5</th>      <td>3. How can the proposed method be generalized to non-image data?</td>    </tr>    <tr>      <th>6</th>      <td>The experiments were only done on simple image datasets.</td>    </tr>    <tr>      <th>7</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>    </tr>    <tr>      <th>8</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>    </tr>    <tr>      <th>9</th>      <td>Minors:</td>    </tr>    <tr>      <th>10</th>      <td>(1) than -&gt; that</td>    </tr>    <tr>      <th>11</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>    </tr>  </tbody></table>            </div>            <div class="column is-half">               <table border="1" class="fancytable">  <thead>    <tr style="text-align: right;">      <th></th>      <th>rebuttal_sentence</th>      <th>sbert_mean</th>      <th>jaccard_mean</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.287781</td>      <td>0.099359</td>    </tr>    <tr>      <th>1</th>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.346934</td>      <td>0.084090</td>    </tr>    <tr>      <th>2</th>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.357485</td>      <td>0.154449</td>    </tr>    <tr>      <th>3</th>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.212782</td>      <td>0.076819</td>    </tr>    <tr>      <th>4</th>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.249911</td>      <td>0.087210</td>    </tr>    <tr>      <th>5</th>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.304595</td>      <td>0.077718</td>    </tr>    <tr>      <th>6</th>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.038567</td>      <td>0.061568</td>    </tr>    <tr>      <th>7</th>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.185645</td>      <td>0.064050</td>    </tr>    <tr>      <th>8</th>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.299544</td>      <td>0.121825</td>    </tr>    <tr>      <th>9</th>      <td>The experiments were only done on simple image datasets.</td>      <td>0.179404</td>      <td>0.120187</td>    </tr>    <tr>      <th>10</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.319136</td>      <td>0.153623</td>    </tr>    <tr>      <th>11</th>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.174988</td>      <td>0.092711</td>    </tr>    <tr>      <th>12</th>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.284971</td>      <td>0.070705</td>    </tr>    <tr>      <th>13</th>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.256614</td>      <td>0.027356</td>    </tr>    <tr>      <th>14</th>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.272596</td>      <td>0.079379</td>    </tr>    <tr>      <th>15</th>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.272684</td>      <td>0.149768</td>    </tr>    <tr>      <th>16</th>      <td>Thank you for pointing this sentence out!</td>      <td>0.071145</td>      <td>0.010812</td>    </tr>    <tr>      <th>17</th>      <td>It is indeed unclear.</td>      <td>0.122979</td>      <td>0.065623</td>    </tr>    <tr>      <th>18</th>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.224945</td>      <td>0.064824</td>    </tr>    <tr>      <th>19</th>      <td>We have updated the draft to be more clear.</td>      <td>0.138625</td>      <td>0.075439</td>    </tr>  </tbody></table>            </div>         </div>         <div class="columns">            <div class="column">               <table border="1" class="fancytable">  <thead>    <tr style="text-align: right;">      <th></th>      <th>review_sentence</th>      <th>rebuttal_sentence</th>      <th>sbert</th>      <th>jaccard</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.877685</td>      <td>0.571429</td>    </tr>    <tr>      <th>1</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.395986</td>      <td>0.046512</td>    </tr>    <tr>      <th>2</th>      <td>More explanation of canonicalization is needed.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.707033</td>      <td>0.055556</td>    </tr>    <tr>      <th>3</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.150288</td>      <td>0.000000</td>    </tr>    <tr>      <th>4</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.223923</td>      <td>0.022727</td>    </tr>    <tr>      <th>5</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.141988</td>      <td>0.041667</td>    </tr>    <tr>      <th>6</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.003280</td>      <td>0.000000</td>    </tr>    <tr>      <th>7</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.132318</td>      <td>0.111111</td>    </tr>    <tr>      <th>8</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.249592</td>      <td>0.192308</td>    </tr>    <tr>      <th>9</th>      <td>Minors:</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.107153</td>      <td>0.076923</td>    </tr>    <tr>      <th>10</th>      <td>(1) than -&gt; that</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.229478</td>      <td>0.000000</td>    </tr>    <tr>      <th>11</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Q: "I had hard time to understand latent canonicalization..."</td>      <td>0.234650</td>      <td>0.074074</td>    </tr>    <tr>      <th>12</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.682672</td>      <td>0.073171</td>    </tr>    <tr>      <th>13</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.627554</td>      <td>0.264151</td>    </tr>    <tr>      <th>14</th>      <td>More explanation of canonicalization is needed.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.752042</td>      <td>0.108108</td>    </tr>    <tr>      <th>15</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.318961</td>      <td>0.075000</td>    </tr>    <tr>      <th>16</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.313070</td>      <td>0.116667</td>    </tr>    <tr>      <th>17</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.248237</td>      <td>0.044444</td>    </tr>    <tr>      <th>18</th>      <td>The experiments were only done on simple image datasets.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.039255</td>      <td>0.023256</td>    </tr>    <tr>      <th>19</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.270814</td>      <td>0.083333</td>    </tr>    <tr>      <th>20</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.208425</td>      <td>0.081633</td>    </tr>    <tr>      <th>21</th>      <td>Minors:</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.206158</td>      <td>0.028571</td>    </tr>    <tr>      <th>22</th>      <td>(1) than -&gt; that</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.274703</td>      <td>0.025641</td>    </tr>    <tr>      <th>23</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>A: "latent canonicalization" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.</td>      <td>0.221313</td>      <td>0.085106</td>    </tr>    <tr>      <th>24</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.281095</td>      <td>0.047619</td>    </tr>    <tr>      <th>25</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.560524</td>      <td>0.135593</td>    </tr>    <tr>      <th>26</th>      <td>More explanation of canonicalization is needed.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.310727</td>      <td>0.078947</td>    </tr>    <tr>      <th>27</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.282705</td>      <td>0.048780</td>    </tr>    <tr>      <th>28</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.975269</td>      <td>0.914286</td>    </tr>    <tr>      <th>29</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.456590</td>      <td>0.205128</td>    </tr>    <tr>      <th>30</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.291026</td>      <td>0.073171</td>    </tr>    <tr>      <th>31</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.459128</td>      <td>0.106383</td>    </tr>    <tr>      <th>32</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.256567</td>      <td>0.104167</td>    </tr>    <tr>      <th>33</th>      <td>Minors:</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.108277</td>      <td>0.028571</td>    </tr>    <tr>      <th>34</th>      <td>(1) than -&gt; that</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.179618</td>      <td>0.025641</td>    </tr>    <tr>      <th>35</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>0.128296</td>      <td>0.085106</td>    </tr>    <tr>      <th>36</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.166303</td>      <td>0.030303</td>    </tr>    <tr>      <th>37</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.174854</td>      <td>0.096154</td>    </tr>    <tr>      <th>38</th>      <td>More explanation of canonicalization is needed.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.227329</td>      <td>0.107143</td>    </tr>    <tr>      <th>39</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.290884</td>      <td>0.100000</td>    </tr>    <tr>      <th>40</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.356742</td>      <td>0.140000</td>    </tr>    <tr>      <th>41</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.362764</td>      <td>0.088235</td>    </tr>    <tr>      <th>42</th>      <td>The experiments were only done on simple image datasets.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.123067</td>      <td>0.030303</td>    </tr>    <tr>      <th>43</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.286127</td>      <td>0.050000</td>    </tr>    <tr>      <th>44</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.269449</td>      <td>0.131579</td>    </tr>    <tr>      <th>45</th>      <td>Minors:</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.150090</td>      <td>0.040000</td>    </tr>    <tr>      <th>46</th>      <td>(1) than -&gt; that</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.027741</td>      <td>0.000000</td>    </tr>    <tr>      <th>47</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.</td>      <td>0.118038</td>      <td>0.108108</td>    </tr>    <tr>      <th>48</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.138935</td>      <td>0.076923</td>    </tr>    <tr>      <th>49</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.287521</td>      <td>0.133333</td>    </tr>    <tr>      <th>50</th>      <td>More explanation of canonicalization is needed.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.276956</td>      <td>0.136364</td>    </tr>    <tr>      <th>51</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.328035</td>      <td>0.080000</td>    </tr>    <tr>      <th>52</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.321966</td>      <td>0.159091</td>    </tr>    <tr>      <th>53</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.316532</td>      <td>0.148148</td>    </tr>    <tr>      <th>54</th>      <td>The experiments were only done on simple image datasets.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.173666</td>      <td>0.037037</td>    </tr>    <tr>      <th>55</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.633845</td>      <td>0.125000</td>    </tr>    <tr>      <th>56</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.175930</td>      <td>0.121212</td>    </tr>    <tr>      <th>57</th>      <td>Minors:</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.146479</td>      <td>0.000000</td>    </tr>    <tr>      <th>58</th>      <td>(1) than -&gt; that</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>0.208808</td>      <td>0.000000</td>    </tr>    <tr>      <th>59</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>In those cases full access to factors of variation is available as this is used to generate the data.</td>      <td>-0.009745</td>      <td>0.029412</td>    </tr>    <tr>      <th>60</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.310919</td>      <td>0.058824</td>    </tr>    <tr>      <th>61</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.433675</td>      <td>0.134615</td>    </tr>    <tr>      <th>62</th>      <td>More explanation of canonicalization is needed.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.408104</td>      <td>0.100000</td>    </tr>    <tr>      <th>63</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.329735</td>      <td>0.060606</td>    </tr>    <tr>      <th>64</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.519729</td>      <td>0.180000</td>    </tr>    <tr>      <th>65</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.368891</td>      <td>0.114286</td>    </tr>    <tr>      <th>66</th>      <td>The experiments were only done on simple image datasets.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.189159</td>      <td>0.090909</td>    </tr>    <tr>      <th>67</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.438178</td>      <td>0.073171</td>    </tr>    <tr>      <th>68</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.276106</td>      <td>0.071429</td>    </tr>    <tr>      <th>69</th>      <td>Minors:</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.077221</td>      <td>0.000000</td>    </tr>    <tr>      <th>70</th>      <td>(1) than -&gt; that</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.220103</td>      <td>0.000000</td>    </tr>    <tr>      <th>71</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>, The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.</td>      <td>0.083316</td>      <td>0.048780</td>    </tr>    <tr>      <th>72</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.114804</td>      <td>0.041667</td>    </tr>    <tr>      <th>73</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.139129</td>      <td>0.043478</td>    </tr>    <tr>      <th>74</th>      <td>More explanation of canonicalization is needed.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.083175</td>      <td>0.047619</td>    </tr>    <tr>      <th>75</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>-0.048178</td>      <td>0.043478</td>    </tr>    <tr>      <th>76</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.151404</td>      <td>0.170732</td>    </tr>    <tr>      <th>77</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>-0.000753</td>      <td>0.076923</td>    </tr>    <tr>      <th>78</th>      <td>The experiments were only done on simple image datasets.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.129545</td>      <td>0.086957</td>    </tr>    <tr>      <th>79</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.030471</td>      <td>0.064516</td>    </tr>    <tr>      <th>80</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>-0.086262</td>      <td>0.096774</td>    </tr>    <tr>      <th>81</th>      <td>Minors:</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>-0.091166</td>      <td>0.000000</td>    </tr>    <tr>      <th>82</th>      <td>(1) than -&gt; that</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>-0.082197</td>      <td>0.000000</td>    </tr>    <tr>      <th>83</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>We are not focusing on a setting where the source domain has no labels.</td>      <td>0.122837</td>      <td>0.066667</td>    </tr>    <tr>      <th>84</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.040491</td>      <td>0.060606</td>    </tr>    <tr>      <th>85</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.155509</td>      <td>0.035714</td>    </tr>    <tr>      <th>86</th>      <td>More explanation of canonicalization is needed.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.089004</td>      <td>0.032258</td>    </tr>    <tr>      <th>87</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.108896</td>      <td>0.062500</td>    </tr>    <tr>      <th>88</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.330215</td>      <td>0.054545</td>    </tr>    <tr>      <th>89</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.299310</td>      <td>0.085714</td>    </tr>    <tr>      <th>90</th>      <td>The experiments were only done on simple image datasets.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.350549</td>      <td>0.060606</td>    </tr>    <tr>      <th>91</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.480985</td>      <td>0.102564</td>    </tr>    <tr>      <th>92</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.167197</td>      <td>0.128205</td>    </tr>    <tr>      <th>93</th>      <td>Minors:</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.067533</td>      <td>0.000000</td>    </tr>    <tr>      <th>94</th>      <td>(1) than -&gt; that</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.113117</td>      <td>0.068966</td>    </tr>    <tr>      <th>95</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).</td>      <td>0.024936</td>      <td>0.076923</td>    </tr>    <tr>      <th>96</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.144363</td>      <td>0.045455</td>    </tr>    <tr>      <th>97</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.205920</td>      <td>0.069767</td>    </tr>    <tr>      <th>98</th>      <td>More explanation of canonicalization is needed.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.223933</td>      <td>0.000000</td>    </tr>    <tr>      <th>99</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.254221</td>      <td>0.000000</td>    </tr>    <tr>      <th>100</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.450230</td>      <td>0.179487</td>    </tr>    <tr>      <th>101</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.940710</td>      <td>0.733333</td>    </tr>    <tr>      <th>102</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.386268</td>      <td>0.000000</td>    </tr>    <tr>      <th>103</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.389238</td>      <td>0.148148</td>    </tr>    <tr>      <th>104</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.206161</td>      <td>0.142857</td>    </tr>    <tr>      <th>105</th>      <td>Minors:</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.139466</td>      <td>0.071429</td>    </tr>    <tr>      <th>106</th>      <td>(1) than -&gt; that</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.107098</td>      <td>0.000000</td>    </tr>    <tr>      <th>107</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Q: How can the proposed method be generalized to non-image data?</td>      <td>0.146924</td>      <td>0.071429</td>    </tr>    <tr>      <th>108</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.055360</td>      <td>0.052632</td>    </tr>    <tr>      <th>109</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.111702</td>      <td>0.000000</td>    </tr>    <tr>      <th>110</th>      <td>More explanation of canonicalization is needed.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.017517</td>      <td>0.062500</td>    </tr>    <tr>      <th>111</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.034859</td>      <td>0.055556</td>    </tr>    <tr>      <th>112</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.307449</td>      <td>0.075000</td>    </tr>    <tr>      <th>113</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.369842</td>      <td>0.045455</td>    </tr>    <tr>      <th>114</th>      <td>The experiments were only done on simple image datasets.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>1.000000</td>      <td>1.000000</td>    </tr>    <tr>      <th>115</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.262081</td>      <td>0.076923</td>    </tr>    <tr>      <th>116</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.001372</td>      <td>0.035714</td>    </tr>    <tr>      <th>117</th>      <td>Minors:</td>      <td>The experiments were only done on simple image datasets.</td>      <td>0.088548</td>      <td>0.000000</td>    </tr>    <tr>      <th>118</th>      <td>(1) than -&gt; that</td>      <td>The experiments were only done on simple image datasets.</td>      <td>-0.065618</td>      <td>0.000000</td>    </tr>    <tr>      <th>119</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>The experiments were only done on simple image datasets.</td>      <td>-0.030264</td>      <td>0.038462</td>    </tr>    <tr>      <th>120</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.196892</td>      <td>0.166667</td>    </tr>    <tr>      <th>121</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.390265</td>      <td>0.062500</td>    </tr>    <tr>      <th>122</th>      <td>More explanation of canonicalization is needed.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.181966</td>      <td>0.041667</td>    </tr>    <tr>      <th>123</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.331474</td>      <td>0.038462</td>    </tr>    <tr>      <th>124</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.462254</td>      <td>0.108696</td>    </tr>    <tr>      <th>125</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.428962</td>      <td>0.192308</td>    </tr>    <tr>      <th>126</th>      <td>The experiments were only done on simple image datasets.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.262081</td>      <td>0.076923</td>    </tr>    <tr>      <th>127</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>1.000000</td>      <td>1.000000</td>    </tr>    <tr>      <th>128</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.280820</td>      <td>0.156250</td>    </tr>    <tr>      <th>129</th>      <td>Minors:</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.187631</td>      <td>0.000000</td>    </tr>    <tr>      <th>130</th>      <td>(1) than -&gt; that</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>0.115077</td>      <td>0.000000</td>    </tr>    <tr>      <th>131</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>-0.007785</td>      <td>0.000000</td>    </tr>    <tr>      <th>132</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.117262</td>      <td>0.064516</td>    </tr>    <tr>      <th>133</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.141969</td>      <td>0.056604</td>    </tr>    <tr>      <th>134</th>      <td>More explanation of canonicalization is needed.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.160828</td>      <td>0.071429</td>    </tr>    <tr>      <th>135</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.086210</td>      <td>0.066667</td>    </tr>    <tr>      <th>136</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.268779</td>      <td>0.166667</td>    </tr>    <tr>      <th>137</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.431589</td>      <td>0.161290</td>    </tr>    <tr>      <th>138</th>      <td>The experiments were only done on simple image datasets.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.302923</td>      <td>0.064516</td>    </tr>    <tr>      <th>139</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.298448</td>      <td>0.171429</td>    </tr>    <tr>      <th>140</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.137462</td>      <td>0.166667</td>    </tr>    <tr>      <th>141</th>      <td>Minors:</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.031375</td>      <td>0.041667</td>    </tr>    <tr>      <th>142</th>      <td>(1) than -&gt; that</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.045927</td>      <td>0.000000</td>    </tr>    <tr>      <th>143</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.</td>      <td>0.077086</td>      <td>0.081081</td>    </tr>    <tr>      <th>144</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.261587</td>      <td>0.050000</td>    </tr>    <tr>      <th>145</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.295783</td>      <td>0.101695</td>    </tr>    <tr>      <th>146</th>      <td>More explanation of canonicalization is needed.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.270123</td>      <td>0.083333</td>    </tr>    <tr>      <th>147</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.360414</td>      <td>0.051282</td>    </tr>    <tr>      <th>148</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.375400</td>      <td>0.120690</td>    </tr>    <tr>      <th>149</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.274706</td>      <td>0.097561</td>    </tr>    <tr>      <th>150</th>      <td>The experiments were only done on simple image datasets.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.314747</td>      <td>0.050000</td>    </tr>    <tr>      <th>151</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.507073</td>      <td>0.086957</td>    </tr>    <tr>      <th>152</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.274392</td>      <td>0.062500</td>    </tr>    <tr>      <th>153</th>      <td>Minors:</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.150431</td>      <td>0.000000</td>    </tr>    <tr>      <th>154</th>      <td>(1) than -&gt; that</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.300618</td>      <td>0.055556</td>    </tr>    <tr>      <th>155</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.</td>      <td>0.034371</td>      <td>0.088889</td>    </tr>    <tr>      <th>156</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.320799</td>      <td>0.000000</td>    </tr>    <tr>      <th>157</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.274676</td>      <td>0.046512</td>    </tr>    <tr>      <th>158</th>      <td>More explanation of canonicalization is needed.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.328994</td>      <td>0.055556</td>    </tr>    <tr>      <th>159</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.200416</td>      <td>0.000000</td>    </tr>    <tr>      <th>160</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.365655</td>      <td>0.071429</td>    </tr>    <tr>      <th>161</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.251793</td>      <td>0.086957</td>    </tr>    <tr>      <th>162</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.239747</td>      <td>0.000000</td>    </tr>    <tr>      <th>163</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.466905</td>      <td>0.034483</td>    </tr>    <tr>      <th>164</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.292498</td>      <td>0.033333</td>    </tr>    <tr>      <th>165</th>      <td>Minors:</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.079263</td>      <td>0.000000</td>    </tr>    <tr>      <th>166</th>      <td>(1) than -&gt; that</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.168215</td>      <td>0.000000</td>    </tr>    <tr>      <th>167</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Critically, these two sources of data need not be identical!</td>      <td>0.090401</td>      <td>0.000000</td>    </tr>    <tr>      <th>168</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.236696</td>      <td>0.055556</td>    </tr>    <tr>      <th>169</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.320050</td>      <td>0.150943</td>    </tr>    <tr>      <th>170</th>      <td>More explanation of canonicalization is needed.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.301444</td>      <td>0.093750</td>    </tr>    <tr>      <th>171</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.398822</td>      <td>0.121212</td>    </tr>    <tr>      <th>172</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.373847</td>      <td>0.109091</td>    </tr>    <tr>      <th>173</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.320391</td>      <td>0.078947</td>    </tr>    <tr>      <th>174</th>      <td>The experiments were only done on simple image datasets.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.194219</td>      <td>0.027027</td>    </tr>    <tr>      <th>175</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.463524</td>      <td>0.095238</td>    </tr>    <tr>      <th>176</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.171688</td>      <td>0.119048</td>    </tr>    <tr>      <th>177</th>      <td>Minors:</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.131599</td>      <td>0.000000</td>    </tr>    <tr>      <th>178</th>      <td>(1) than -&gt; that</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.238966</td>      <td>0.030303</td>    </tr>    <tr>      <th>179</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.</td>      <td>0.119904</td>      <td>0.071429</td>    </tr>    <tr>      <th>180</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.298637</td>      <td>0.160000</td>    </tr>    <tr>      <th>181</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.285269</td>      <td>0.061224</td>    </tr>    <tr>      <th>182</th>      <td>More explanation of canonicalization is needed.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.230615</td>      <td>0.040000</td>    </tr>    <tr>      <th>183</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.220907</td>      <td>0.076923</td>    </tr>    <tr>      <th>184</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.276710</td>      <td>0.083333</td>    </tr>    <tr>      <th>185</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.245067</td>      <td>0.142857</td>    </tr>    <tr>      <th>186</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.008097</td>      <td>0.035714</td>    </tr>    <tr>      <th>187</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.289637</td>      <td>0.156250</td>    </tr>    <tr>      <th>188</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.934402</td>      <td>0.900000</td>    </tr>    <tr>      <th>189</th>      <td>Minors:</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.087201</td>      <td>0.050000</td>    </tr>    <tr>      <th>190</th>      <td>(1) than -&gt; that</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.326071</td>      <td>0.000000</td>    </tr>    <tr>      <th>191</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Q: I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>0.069599</td>      <td>0.090909</td>    </tr>    <tr>      <th>192</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.157651</td>      <td>0.000000</td>    </tr>    <tr>      <th>193</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.108067</td>      <td>0.051282</td>    </tr>    <tr>      <th>194</th>      <td>More explanation of canonicalization is needed.</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.163446</td>      <td>0.000000</td>    </tr>    <tr>      <th>195</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.089062</td>      <td>0.000000</td>    </tr>    <tr>      <th>196</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>Thank you for pointing this sentence out!</td>      <td>-0.013151</td>      <td>0.000000</td>    </tr>    <tr>      <th>197</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.007578</td>      <td>0.000000</td>    </tr>    <tr>      <th>198</th>      <td>The experiments were only done on simple image datasets.</td>      <td>Thank you for pointing this sentence out!</td>      <td>-0.127593</td>      <td>0.000000</td>    </tr>    <tr>      <th>199</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.009175</td>      <td>0.040000</td>    </tr>    <tr>      <th>200</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.097536</td>      <td>0.038462</td>    </tr>    <tr>      <th>201</th>      <td>Minors:</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.185080</td>      <td>0.000000</td>    </tr>    <tr>      <th>202</th>      <td>(1) than -&gt; that</td>      <td>Thank you for pointing this sentence out!</td>      <td>0.178880</td>      <td>0.000000</td>    </tr>    <tr>      <th>203</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>Thank you for pointing this sentence out!</td>      <td>-0.001990</td>      <td>0.000000</td>    </tr>    <tr>      <th>204</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>It is indeed unclear.</td>      <td>0.216047</td>      <td>0.071429</td>    </tr>    <tr>      <th>205</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>It is indeed unclear.</td>      <td>0.056524</td>      <td>0.027027</td>    </tr>    <tr>      <th>206</th>      <td>More explanation of canonicalization is needed.</td>      <td>It is indeed unclear.</td>      <td>0.127763</td>      <td>0.200000</td>    </tr>    <tr>      <th>207</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>It is indeed unclear.</td>      <td>0.144536</td>      <td>0.166667</td>    </tr>    <tr>      <th>208</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>It is indeed unclear.</td>      <td>0.101963</td>      <td>0.055556</td>    </tr>    <tr>      <th>209</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>It is indeed unclear.</td>      <td>0.001547</td>      <td>0.058824</td>    </tr>    <tr>      <th>210</th>      <td>The experiments were only done on simple image datasets.</td>      <td>It is indeed unclear.</td>      <td>0.166278</td>      <td>0.071429</td>    </tr>    <tr>      <th>211</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>It is indeed unclear.</td>      <td>0.148488</td>      <td>0.045455</td>    </tr>    <tr>      <th>212</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>It is indeed unclear.</td>      <td>0.107521</td>      <td>0.043478</td>    </tr>    <tr>      <th>213</th>      <td>Minors:</td>      <td>It is indeed unclear.</td>      <td>0.158219</td>      <td>0.000000</td>    </tr>    <tr>      <th>214</th>      <td>(1) than -&gt; that</td>      <td>It is indeed unclear.</td>      <td>0.187958</td>      <td>0.000000</td>    </tr>    <tr>      <th>215</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>It is indeed unclear.</td>      <td>0.058905</td>      <td>0.047619</td>    </tr>    <tr>      <th>216</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.238431</td>      <td>0.074074</td>    </tr>    <tr>      <th>217</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.394029</td>      <td>0.106383</td>    </tr>    <tr>      <th>218</th>      <td>More explanation of canonicalization is needed.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.197374</td>      <td>0.083333</td>    </tr>    <tr>      <th>219</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.192083</td>      <td>0.076923</td>    </tr>    <tr>      <th>220</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.282242</td>      <td>0.083333</td>    </tr>    <tr>      <th>221</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.174590</td>      <td>0.066667</td>    </tr>    <tr>      <th>222</th>      <td>The experiments were only done on simple image datasets.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.143681</td>      <td>0.074074</td>    </tr>    <tr>      <th>223</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.317296</td>      <td>0.057143</td>    </tr>    <tr>      <th>224</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.374840</td>      <td>0.085714</td>    </tr>    <tr>      <th>225</th>      <td>Minors:</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.222706</td>      <td>0.000000</td>    </tr>    <tr>      <th>226</th>      <td>(1) than -&gt; that</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>0.163140</td>      <td>0.041667</td>    </tr>    <tr>      <th>227</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>All we were trying to say is that each baselineâ€™s training duration was chosen independently to prevent overfitting.</td>      <td>-0.001070</td>      <td>0.028571</td>    </tr>    <tr>      <th>228</th>      <td>1. I had hard time to understand latent canonicalization.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.171903</td>      <td>0.111111</td>    </tr>    <tr>      <th>229</th>      <td>Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?</td>      <td>We have updated the draft to be more clear.</td>      <td>0.235569</td>      <td>0.075000</td>    </tr>    <tr>      <th>230</th>      <td>More explanation of canonicalization is needed.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.258380</td>      <td>0.062500</td>    </tr>    <tr>      <th>231</th>      <td>Perhaps an example in linear algebra is needed.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.062374</td>      <td>0.055556</td>    </tr>    <tr>      <th>232</th>      <td>2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?</td>      <td>We have updated the draft to be more clear.</td>      <td>0.203951</td>      <td>0.102564</td>    </tr>    <tr>      <th>233</th>      <td>3. How can the proposed method be generalized to non-image data?</td>      <td>We have updated the draft to be more clear.</td>      <td>0.195894</td>      <td>0.210526</td>    </tr>    <tr>      <th>234</th>      <td>The experiments were only done on simple image datasets.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.014477</td>      <td>0.052632</td>    </tr>    <tr>      <th>235</th>      <td>I am wondering this method can be applied to other complex datasets whose latent factors are unknown.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.043682</td>      <td>0.120000</td>    </tr>    <tr>      <th>236</th>      <td>4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.</td>      <td>We have updated the draft to be more clear.</td>      <td>0.179615</td>      <td>0.115385</td>    </tr>    <tr>      <th>237</th>      <td>Minors:</td>      <td>We have updated the draft to be more clear.</td>      <td>0.108070</td>      <td>0.000000</td>    </tr>    <tr>      <th>238</th>      <td>(1) than -&gt; that</td>      <td>We have updated the draft to be more clear.</td>      <td>0.113155</td>      <td>0.000000</td>    </tr>    <tr>      <th>239</th>      <td>(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?</td>      <td>We have updated the draft to be more clear.</td>      <td>0.076432</td>      <td>0.000000</td>    </tr>  </tbody></table>            </div>         </div>      </div>      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>      <script src="https://cdn.datatables.net/1.12.0/js/jquery.dataTables.min.js"></script>      <script src="js/dataTables.bulma.min.js"></script>      <script type="text/javascript">         $(".fancytable").DataTable();      </script>   </body></HTML>